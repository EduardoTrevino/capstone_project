{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting report generation script...\n",
      "Fetching data for learners starting with 'BT'...\n",
      "An error occurred while fetching data: BaseSelectRequestBuilder.order() got an unexpected keyword argument 'ascending'\n",
      "Exiting script as no data was fetched.\n"
     ]
    }
   ],
   "source": [
    "# generate_reports.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data and DB Libraries\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Reporting and Plotting Libraries\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The folder where all reports will be saved\n",
    "OUTPUT_DIR = Path(\"GeneratedReports\")\n",
    "# The prefix for learners we are interested in\n",
    "LEARNER_PREFIX = \"BT\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def add_df_to_doc(doc, df, title=\"\"):\n",
    "    \"\"\"Adds a Pandas DataFrame to a docx document with a title.\"\"\"\n",
    "    if title:\n",
    "        doc.add_heading(title, level=2)\n",
    "    \n",
    "    # Add a table to the document\n",
    "    if df.empty:\n",
    "        doc.add_paragraph(\"No data available.\")\n",
    "        return\n",
    "        \n",
    "    # Reset index if it's not meaningful (like 0, 1, 2, ...)\n",
    "    if isinstance(df.index, pd.RangeIndex):\n",
    "        df_for_table = df.reset_index(drop=True)\n",
    "    else:\n",
    "        df_for_table = df.reset_index()\n",
    "\n",
    "    table = doc.add_table(rows=1, cols=len(df_for_table.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    \n",
    "    # Add the header rows.\n",
    "    for j, col_name in enumerate(df_for_table.columns):\n",
    "        table.cell(0, j).text = str(col_name)\n",
    "\n",
    "    # Add the rest of the data frame\n",
    "    for i, row in df_for_table.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, val in enumerate(row):\n",
    "            row_cells[j].text = str(val)\n",
    "\n",
    "def create_plot_from_df(df, title, y_label, x_col='Decision Point'):\n",
    "    \"\"\"Creates a line plot from a dataframe and returns it as an in-memory image.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != x_col:\n",
    "            ax.plot(df[x_col], df[col], marker='o', linestyle='-', label=col)\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_col, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    fig.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make room for legend\n",
    "    \n",
    "    # Save plot to an in-memory buffer\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=300)\n",
    "    plt.close(fig) # Close the plot to free memory\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "\n",
    "# --- CORE SCRIPT LOGIC ---\n",
    "\n",
    "def fetch_analytics_data(supabase: Client) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and cleans data from the historical_learning_analytics table.\"\"\"\n",
    "    print(f\"Fetching data for learners starting with '{LEARNER_PREFIX}'...\")\n",
    "    try:\n",
    "        response = supabase.table('historical_learning_analytics').select(\n",
    "            'created_at, user_id, goal_id, scenario_attempt_number, decision_number, kc_scores_after_decision, metric_values_after_decision, users!inner(name)'\n",
    "        ).like(\n",
    "            'users.name', f'{LEARNER_PREFIX}%'\n",
    "        ).order(\n",
    "            'user_id'\n",
    "        ).order(\n",
    "            'created_at', desc=False  # <-- THIS IS THE CORRECTED LINE\n",
    "        ).execute()\n",
    "\n",
    "        if not response.data:\n",
    "            print(\"No data found for the specified learners.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(response.data)\n",
    "        \n",
    "        # Unpack user name from the nested dictionary\n",
    "        df['user_name'] = df['users'].apply(lambda x: x['name'])\n",
    "        df = df.drop(columns=['users'])\n",
    "        \n",
    "        print(f\"Successfully fetched {len(df)} records.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_learner_data(raw_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Processes the raw DataFrame into a dictionary of DataFrames, one for each learner.\"\"\"\n",
    "    print(\"Processing raw data...\")\n",
    "    if raw_df.empty:\n",
    "        return {}\n",
    "\n",
    "    all_learners_data = {}\n",
    "    \n",
    "    # Get all unique KC and Metric names across the entire dataset\n",
    "    all_kcs = set()\n",
    "    all_metrics = set()\n",
    "    raw_df['kc_scores_after_decision'].dropna().apply(lambda x: all_kcs.update(x.keys()))\n",
    "    raw_df['metric_values_after_decision'].dropna().apply(lambda x: all_metrics.update(x.keys()))\n",
    "\n",
    "    # Group data by each user\n",
    "    for user_id, group in raw_df.groupby('user_id'):\n",
    "        user_name = group['user_name'].iloc[0]\n",
    "        print(f\"  -> Processing data for {user_name} ({user_id})\")\n",
    "\n",
    "        # Sort chronologically for this user\n",
    "        learner_df = group.sort_values('created_at').reset_index(drop=True)\n",
    "        \n",
    "        # Create a clean 'Decision Point' identifier for plotting\n",
    "        learner_df['Decision Point'] = [f\"G{r.goal_id}-S{r.scenario_attempt_number}-D{r.decision_number}\" for r in learner_df.itertuples()]\n",
    "\n",
    "        # Expand the JSON columns into actual dataframe columns\n",
    "        kc_df = pd.json_normalize(learner_df['kc_scores_after_decision']).reindex(columns=sorted(list(all_kcs))).fillna(0)\n",
    "        metric_df = pd.json_normalize(learner_df['metric_values_after_decision']).reindex(columns=sorted(list(all_metrics))).fillna(0)\n",
    "\n",
    "        # Combine everything into a single, clean DataFrame for the learner\n",
    "        processed_df = pd.concat([\n",
    "            learner_df[['user_id', 'user_name', 'created_at', 'Decision Point']],\n",
    "            kc_df,\n",
    "            metric_df\n",
    "        ], axis=1)\n",
    "\n",
    "        all_learners_data[user_id] = {\n",
    "            \"user_name\": user_name,\n",
    "            \"full_data\": processed_df,\n",
    "            \"all_kcs\": sorted(list(all_kcs)),\n",
    "            \"all_metrics\": sorted(list(all_metrics))\n",
    "        }\n",
    "        \n",
    "    print(\"Data processing complete.\")\n",
    "    return all_learners_data\n",
    "\n",
    "\n",
    "def generate_individual_report(learner_data: dict, output_dir: Path):\n",
    "    \"\"\"Generates a single Word document report for one learner.\"\"\"\n",
    "    user_name = learner_data['user_name']\n",
    "    df = learner_data['full_data']\n",
    "    kcs = learner_data['all_kcs']\n",
    "    metrics = learner_data['all_metrics']\n",
    "    \n",
    "    print(f\"  -> Generating report for {user_name}...\")\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Learner Report: {user_name}', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # --- Section 1: Final Scores ---\n",
    "    doc.add_heading('Final Cumulative Scores', level=1)\n",
    "    final_scores = df.iloc[-1]\n",
    "    \n",
    "    # Final KC Scores Table\n",
    "    final_kc_scores = final_scores[kcs].to_frame(name='Final Score')\n",
    "    final_kc_scores.index.name = \"Knowledge Component (KC)\"\n",
    "    add_df_to_doc(doc, final_kc_scores, \"Final KC Scores\")\n",
    "\n",
    "    # Final Metric Scores Table\n",
    "    final_metric_scores = final_scores[metrics].to_frame(name='Final Value')\n",
    "    final_metric_scores.index.name = \"Metric\"\n",
    "    add_df_to_doc(doc, final_metric_scores, \"Final Metric Scores\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "\n",
    "    # --- Section 2: Scores Over Time ---\n",
    "    doc.add_heading('Performance Over Time', level=1)\n",
    "    \n",
    "    # KC Scores Progression\n",
    "    kc_progression_df = df[['Decision Point'] + kcs]\n",
    "    add_df_to_doc(doc, kc_progression_df.set_index('Decision Point'), \"KC Scores per Decision Point\")\n",
    "    \n",
    "    kc_plot_img = create_plot_from_df(kc_progression_df, \"KC Scores Over Time\", \"KC Score\")\n",
    "    doc.add_picture(kc_plot_img, width=Inches(6.5))\n",
    "    \n",
    "    # Metric Scores Progression\n",
    "    metric_progression_df = df[['Decision Point'] + metrics]\n",
    "    add_df_to_doc(doc, metric_progression_df.set_index('Decision Point'), \"Metric Values per Decision Point\")\n",
    "    \n",
    "    metric_plot_img = create_plot_from_df(metric_progression_df, \"Metric Values Over Time\", \"Metric Value\")\n",
    "    doc.add_picture(metric_plot_img, width=Inches(6.5))\n",
    "\n",
    "    # --- Save the document ---\n",
    "    file_path = output_dir / f\"Report_{user_name.replace(' ', '_')}.docx\"\n",
    "    doc.save(file_path)\n",
    "\n",
    "\n",
    "def generate_master_report(all_learners_data: dict, output_dir: Path):\n",
    "    \"\"\"Generates the master report with aggregate stats and leaderboards.\"\"\"\n",
    "    print(\"-> Generating Master Report...\")\n",
    "    if not all_learners_data:\n",
    "        print(\"   No data available to generate a master report.\")\n",
    "        return\n",
    "\n",
    "    doc = Document()\n",
    "    doc.add_heading('Master Learner Report', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(f\"This report covers {len(all_learners_data)} learners whose names start with '{LEARNER_PREFIX}'.\")\n",
    "\n",
    "    # --- Compile final scores from all learners into one DataFrame ---\n",
    "    final_scores_list = []\n",
    "    for user_id, data in all_learners_data.items():\n",
    "        final_row = data['full_data'].iloc[-1].copy()\n",
    "        final_scores_list.append(final_row)\n",
    "    \n",
    "    final_scores_df = pd.DataFrame(final_scores_list).set_index('user_name')\n",
    "    kcs = all_learners_data[list(all_learners_data.keys())[0]]['all_kcs']\n",
    "    metrics = all_learners_data[list(all_learners_data.keys())[0]]['all_metrics']\n",
    "\n",
    "    # --- Section 1: Aggregate Statistics ---\n",
    "    doc.add_heading('Aggregate Statistics (Final Scores)', level=1)\n",
    "    \n",
    "    # KC Stats\n",
    "    kc_stats = final_scores_df[kcs].describe().loc[['mean', 'min', 'max']].round(2)\n",
    "    add_df_to_doc(doc, kc_stats, \"KC Score Statistics (Across All Learners)\")\n",
    "\n",
    "    # Metric Stats\n",
    "    metric_stats = final_scores_df[metrics].describe().loc[['mean', 'min', 'max']].round(2)\n",
    "    add_df_to_doc(doc, metric_stats, \"Metric Value Statistics (Across All Learners)\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "    \n",
    "    # --- Section 2: Leaderboards ---\n",
    "    doc.add_heading('Leaderboards', level=1)\n",
    "    \n",
    "    # KC Leaderboard (based on sum of all final KC scores)\n",
    "    final_scores_df['Total KC Score'] = final_scores_df[kcs].sum(axis=1)\n",
    "    kc_leaderboard = final_scores_df[['Total KC Score']].sort_values('Total KC Score', ascending=False).round(2)\n",
    "    add_df_to_doc(doc, kc_leaderboard.head(10), \"Top 10 - KC Leaderboard (by Sum of Final KC Scores)\")\n",
    "    \n",
    "    # Metric Leaderboard (based on a normalized composite score)\n",
    "    # Normalizing is important because metrics have different scales (e.g., Revenue vs. Reputation)\n",
    "    # We scale each metric to be between 0 and 1, then sum them up for a composite score.\n",
    "    doc.add_paragraph(\n",
    "        \"The Metric Leaderboard is calculated by normalizing each metric (scaling from 0 to 1 based on the min/max in this group) \"\n",
    "        \"and then summing these normalized scores. This gives a balanced view of overall performance across all metrics.\"\n",
    "    )\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_metrics = scaler.fit_transform(final_scores_df[metrics])\n",
    "    final_scores_df['Composite Metric Score'] = pd.DataFrame(normalized_metrics, index=final_scores_df.index).sum(axis=1)\n",
    "    metric_leaderboard = final_scores_df[['Composite Metric Score']].sort_values('Composite Metric Score', ascending=False).round(3)\n",
    "    add_df_to_doc(doc, metric_leaderboard.head(10), \"Top 10 - Metric Leaderboard (by Composite Score)\")\n",
    "\n",
    "    # --- Save the document ---\n",
    "    file_path = output_dir / \"Master_Report.docx\"\n",
    "    doc.save(file_path)\n",
    "    print(\"   Master Report saved.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire reporting script.\"\"\"\n",
    "    # --- Setup ---\n",
    "    print(\"Starting report generation script...\")\n",
    "    load_dotenv('.env.local')\n",
    "    \n",
    "    supabase_url = os.environ.get(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "    supabase_key = os.environ.get(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "\n",
    "    if not supabase_url or not supabase_key:\n",
    "        print(\"Error: Supabase URL or Key not found in .env.local file.\")\n",
    "        print(\"Please create a .env.local file with your credentials.\")\n",
    "        return\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize Supabase client\n",
    "    supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "    # --- Data Fetching and Processing ---\n",
    "    raw_df = fetch_analytics_data(supabase)\n",
    "    if raw_df.empty:\n",
    "        print(\"Exiting script as no data was fetched.\")\n",
    "        return\n",
    "        \n",
    "    all_learners_data = process_learner_data(raw_df)\n",
    "    \n",
    "    if not all_learners_data:\n",
    "        print(\"Exiting script as no learner data could be processed.\")\n",
    "        return\n",
    "\n",
    "    # --- Report Generation ---\n",
    "    print(\"\\nGenerating individual learner reports...\")\n",
    "    for user_id in all_learners_data:\n",
    "        generate_individual_report(all_learners_data[user_id], OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\nGenerating master report...\")\n",
    "    generate_master_report(all_learners_data, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\n✅ All reports have been generated and saved in the '{OUTPUT_DIR}' folder.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
