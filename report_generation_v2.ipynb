{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting report generation script...\n",
      "Fetching data for learners starting with 'BT'...\n",
      "Successfully fetched 107 records.\n",
      "Processing raw data...\n",
      "  -> Processing data for BT8BT9 (0dcaf179-58b2-411b-8874-d41c18467735)\n",
      "  -> Processing data for BT15BT16 (1193bb01-6be2-4e97-b603-dda1b811446d)\n",
      "  -> Processing data for BT14 (14b9c968-dc54-4752-a254-8351e461117a)\n",
      "  -> Processing data for BT2 (3184fa5a-11bf-4526-b34b-cfaae969ab29)\n",
      "  -> Processing data for BT4 (3541ed59-254e-41b8-b208-f4c9f7bdb95a)\n",
      "  -> Processing data for BT13 (50730893-f080-4f52-94cb-8b4d438609b8)\n",
      "  -> Processing data for BT7 (63eb2958-1758-4f6d-89f5-2158ac6873f5)\n",
      "  -> Processing data for BT1 (65c037c6-9bdb-4e0d-9f18-ac5d3f7281c9)\n",
      "  -> Processing data for BT11BT12 (701c22f8-fd72-4f51-b46e-d0cadb761db8)\n",
      "  -> Processing data for BT7 (9b65c028-a111-4b49-9bae-1154f9c72326)\n",
      "  -> Processing data for BT5 (bb07dc77-3422-472a-8de3-4cc4ef2fd088)\n",
      "  -> Processing data for BT3+BT6 (d276ef1e-e84d-4b91-b05c-9a55e41d93f5)\n",
      "  -> Processing data for BT10 (d93c7e0c-87a5-4e28-935b-54aa7a9dbb5d)\n",
      "Data processing complete.\n",
      "\n",
      "Generating individual learner reports...\n",
      "  -> Generating report for BT8BT9...\n",
      "  -> Generating report for BT15BT16...\n",
      "  -> Generating report for BT14...\n",
      "  -> Generating report for BT2...\n",
      "  -> Generating report for BT4...\n",
      "  -> Generating report for BT13...\n",
      "  -> Generating report for BT7...\n",
      "  -> Generating report for BT1...\n",
      "  -> Generating report for BT11BT12...\n",
      "  -> Generating report for BT7...\n",
      "  -> Generating report for BT5...\n",
      "  -> Generating report for BT3+BT6...\n",
      "  -> Generating report for BT10...\n",
      "\n",
      "Generating master report...\n",
      "-> Generating Master Report...\n",
      "   Master Report saved.\n",
      "\n",
      "âœ… All reports have been generated and saved in the 'GeneratedReports' folder.\n"
     ]
    }
   ],
   "source": [
    "# generate_reports.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Data and DB Libraries\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Reporting and Plotting Libraries\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The folder where all reports will be saved\n",
    "OUTPUT_DIR = Path(\"GeneratedReports\")\n",
    "# The prefix for learners we are interested in\n",
    "LEARNER_PREFIX = \"BT\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def add_df_to_doc(doc, df, title=\"\"):\n",
    "    \"\"\"Adds a Pandas DataFrame to a docx document with a title.\"\"\"\n",
    "    if title:\n",
    "        doc.add_heading(title, level=2)\n",
    "    \n",
    "    # Add a table to the document\n",
    "    if df.empty:\n",
    "        doc.add_paragraph(\"No data available.\")\n",
    "        return\n",
    "        \n",
    "    # Reset index if it's not meaningful (like 0, 1, 2, ...)\n",
    "    if isinstance(df.index, pd.RangeIndex):\n",
    "        df_for_table = df.reset_index(drop=True)\n",
    "    else:\n",
    "        df_for_table = df.reset_index()\n",
    "\n",
    "    table = doc.add_table(rows=1, cols=len(df_for_table.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    \n",
    "    # Add the header rows.\n",
    "    for j, col_name in enumerate(df_for_table.columns):\n",
    "        table.cell(0, j).text = str(col_name)\n",
    "\n",
    "    # Add the rest of the data frame\n",
    "    for i, row in df_for_table.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, val in enumerate(row):\n",
    "            row_cells[j].text = str(val)\n",
    "\n",
    "def create_plot_from_df(df, title, y_label, x_col='Decision Point'):\n",
    "    \"\"\"Creates a line plot from a dataframe and returns it as an in-memory image.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col != x_col:\n",
    "            ax.plot(df[x_col], df[col], marker='o', linestyle='-', label=col)\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_col, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    fig.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make room for legend\n",
    "    \n",
    "    # Save plot to an in-memory buffer\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=300)\n",
    "    plt.close(fig) # Close the plot to free memory\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "\n",
    "# --- CORE SCRIPT LOGIC ---\n",
    "\n",
    "def fetch_analytics_data(supabase: Client) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and cleans data from the historical_learning_analytics table.\"\"\"\n",
    "    print(f\"Fetching data for learners starting with '{LEARNER_PREFIX}'...\")\n",
    "    try:\n",
    "        response = supabase.table('historical_learning_analytics').select(\n",
    "            'created_at, user_id, goal_id, scenario_attempt_number, decision_number, kc_scores_after_decision, metric_values_after_decision, users!inner(name)'\n",
    "        ).like(\n",
    "            'users.name', f'{LEARNER_PREFIX}%'\n",
    "        ).order(\n",
    "            'user_id'\n",
    "        ).order(\n",
    "            'created_at', desc=False  # <-- THIS IS THE CORRECTED LINE\n",
    "        ).execute()\n",
    "\n",
    "        if not response.data:\n",
    "            print(\"No data found for the specified learners.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(response.data)\n",
    "        \n",
    "        # Unpack user name from the nested dictionary\n",
    "        df['user_name'] = df['users'].apply(lambda x: x['name'])\n",
    "        df = df.drop(columns=['users'])\n",
    "        \n",
    "        print(f\"Successfully fetched {len(df)} records.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def process_learner_data(raw_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Processes the raw DataFrame into a dictionary of DataFrames, one for each learner.\"\"\"\n",
    "    print(\"Processing raw data...\")\n",
    "    if raw_df.empty:\n",
    "        return {}\n",
    "\n",
    "    all_learners_data = {}\n",
    "    \n",
    "    # Get all unique KC and Metric names across the entire dataset\n",
    "    all_kcs = set()\n",
    "    all_metrics = set()\n",
    "    raw_df['kc_scores_after_decision'].dropna().apply(lambda x: all_kcs.update(x.keys()))\n",
    "    raw_df['metric_values_after_decision'].dropna().apply(lambda x: all_metrics.update(x.keys()))\n",
    "\n",
    "    # Group data by each user\n",
    "    for user_id, group in raw_df.groupby('user_id'):\n",
    "        user_name = group['user_name'].iloc[0]\n",
    "        print(f\"  -> Processing data for {user_name} ({user_id})\")\n",
    "\n",
    "        # Sort chronologically for this user\n",
    "        learner_df = group.sort_values('created_at').reset_index(drop=True)\n",
    "        \n",
    "        # Create a clean 'Decision Point' identifier for plotting\n",
    "        learner_df['Decision Point'] = [f\"G{r.goal_id}-S{r.scenario_attempt_number}-D{r.decision_number}\" for r in learner_df.itertuples()]\n",
    "\n",
    "        # Expand the JSON columns into actual dataframe columns\n",
    "        kc_df = pd.json_normalize(learner_df['kc_scores_after_decision']).reindex(columns=sorted(list(all_kcs))).fillna(0)\n",
    "        metric_df = pd.json_normalize(learner_df['metric_values_after_decision']).reindex(columns=sorted(list(all_metrics))).fillna(0)\n",
    "\n",
    "        # Combine everything into a single, clean DataFrame for the learner\n",
    "        processed_df = pd.concat([\n",
    "            learner_df[['user_id', 'user_name', 'created_at', 'Decision Point']],\n",
    "            kc_df,\n",
    "            metric_df\n",
    "        ], axis=1)\n",
    "\n",
    "        all_learners_data[user_id] = {\n",
    "            \"user_name\": user_name,\n",
    "            \"full_data\": processed_df,\n",
    "            \"all_kcs\": sorted(list(all_kcs)),\n",
    "            \"all_metrics\": sorted(list(all_metrics))\n",
    "        }\n",
    "        \n",
    "    print(\"Data processing complete.\")\n",
    "    return all_learners_data\n",
    "\n",
    "\n",
    "def generate_individual_report(learner_data: dict, output_dir: Path):\n",
    "    \"\"\"Generates a single Word document report for one learner.\"\"\"\n",
    "    user_name = learner_data['user_name']\n",
    "    df = learner_data['full_data']\n",
    "    kcs = learner_data['all_kcs']\n",
    "    metrics = learner_data['all_metrics']\n",
    "    \n",
    "    print(f\"  -> Generating report for {user_name}...\")\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Learner Report: {user_name}', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # --- Section 1: Final Scores ---\n",
    "    doc.add_heading('Final Cumulative Scores', level=1)\n",
    "    final_scores = df.iloc[-1]\n",
    "    \n",
    "    # Final KC Scores Table\n",
    "    final_kc_scores = final_scores[kcs].to_frame(name='Final Score')\n",
    "    final_kc_scores.index.name = \"Knowledge Component (KC)\"\n",
    "    add_df_to_doc(doc, final_kc_scores, \"Final KC Scores\")\n",
    "\n",
    "    # Final Metric Scores Table\n",
    "    final_metric_scores = final_scores[metrics].to_frame(name='Final Value')\n",
    "    final_metric_scores.index.name = \"Metric\"\n",
    "    add_df_to_doc(doc, final_metric_scores, \"Final Metric Scores\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "\n",
    "    # --- Section 2: Scores Over Time ---\n",
    "    doc.add_heading('Performance Over Time', level=1)\n",
    "    \n",
    "    # KC Scores Progression\n",
    "    kc_progression_df = df[['Decision Point'] + kcs]\n",
    "    add_df_to_doc(doc, kc_progression_df.set_index('Decision Point'), \"KC Scores per Decision Point\")\n",
    "    \n",
    "    kc_plot_img = create_plot_from_df(kc_progression_df, \"KC Scores Over Time\", \"KC Score\")\n",
    "    doc.add_picture(kc_plot_img, width=Inches(6.5))\n",
    "    \n",
    "    # Metric Scores Progression\n",
    "    metric_progression_df = df[['Decision Point'] + metrics]\n",
    "    add_df_to_doc(doc, metric_progression_df.set_index('Decision Point'), \"Metric Values per Decision Point\")\n",
    "    \n",
    "    metric_plot_img = create_plot_from_df(metric_progression_df, \"Metric Values Over Time\", \"Metric Value\")\n",
    "    doc.add_picture(metric_plot_img, width=Inches(6.5))\n",
    "\n",
    "    # --- Save the document ---\n",
    "    file_path = output_dir / f\"Report_{user_name.replace(' ', '_')}.docx\"\n",
    "    doc.save(file_path)\n",
    "\n",
    "\n",
    "def generate_master_report(all_learners_data: dict, output_dir: Path):\n",
    "    \"\"\"Generates the master report with aggregate stats and leaderboards.\"\"\"\n",
    "    print(\"-> Generating Master Report...\")\n",
    "    if not all_learners_data:\n",
    "        print(\"   No data available to generate a master report.\")\n",
    "        return\n",
    "\n",
    "    doc = Document()\n",
    "    doc.add_heading('Master Learner Report', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(f\"This report covers {len(all_learners_data)} learners whose names start with '{LEARNER_PREFIX}'.\")\n",
    "\n",
    "    # --- Compile final scores from all learners into one DataFrame ---\n",
    "    final_scores_list = []\n",
    "    for user_id, data in all_learners_data.items():\n",
    "        final_row = data['full_data'].iloc[-1].copy()\n",
    "        final_scores_list.append(final_row)\n",
    "    \n",
    "    final_scores_df = pd.DataFrame(final_scores_list).set_index('user_name')\n",
    "    kcs = all_learners_data[list(all_learners_data.keys())[0]]['all_kcs']\n",
    "    metrics = all_learners_data[list(all_learners_data.keys())[0]]['all_metrics']\n",
    "\n",
    "    # --- Section 1: Aggregate Statistics ---\n",
    "    doc.add_heading('Aggregate Statistics (Final Scores)', level=1)\n",
    "    \n",
    "    # KC Stats\n",
    "    kc_stats = final_scores_df[kcs].describe().loc[['mean', 'min', 'max']].round(2)\n",
    "    add_df_to_doc(doc, kc_stats, \"KC Score Statistics (Across All Learners)\")\n",
    "\n",
    "    # Metric Stats\n",
    "    metric_stats = final_scores_df[metrics].describe().loc[['mean', 'min', 'max']].round(2)\n",
    "    add_df_to_doc(doc, metric_stats, \"Metric Value Statistics (Across All Learners)\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "    \n",
    "    # --- Section 2: Leaderboards ---\n",
    "    doc.add_heading('Leaderboards', level=1)\n",
    "    \n",
    "    # KC Leaderboard (based on sum of all final KC scores)\n",
    "    final_scores_df['Total KC Score'] = final_scores_df[kcs].sum(axis=1)\n",
    "    kc_leaderboard = final_scores_df[['Total KC Score']].sort_values('Total KC Score', ascending=False).round(2)\n",
    "    add_df_to_doc(doc, kc_leaderboard.head(10), \"Top 10 - KC Leaderboard (by Sum of Final KC Scores)\")\n",
    "    \n",
    "    # Metric Leaderboard (based on a normalized composite score)\n",
    "    # Normalizing is important because metrics have different scales (e.g., Revenue vs. Reputation)\n",
    "    # We scale each metric to be between 0 and 1, then sum them up for a composite score.\n",
    "    doc.add_paragraph(\n",
    "        \"The Metric Leaderboard is calculated by normalizing each metric (scaling from 0 to 1 based on the min/max in this group) \"\n",
    "        \"and then summing these normalized scores. This gives a balanced view of overall performance across all metrics.\"\n",
    "    )\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_metrics = scaler.fit_transform(final_scores_df[metrics])\n",
    "    final_scores_df['Composite Metric Score'] = pd.DataFrame(normalized_metrics, index=final_scores_df.index).sum(axis=1)\n",
    "    metric_leaderboard = final_scores_df[['Composite Metric Score']].sort_values('Composite Metric Score', ascending=False).round(3)\n",
    "    add_df_to_doc(doc, metric_leaderboard.head(10), \"Top 10 - Metric Leaderboard (by Composite Score)\")\n",
    "\n",
    "    # --- Save the document ---\n",
    "    file_path = output_dir / \"Master_Report.docx\"\n",
    "    doc.save(file_path)\n",
    "    print(\"   Master Report saved.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire reporting script.\"\"\"\n",
    "    # --- Setup ---\n",
    "    print(\"Starting report generation script...\")\n",
    "    load_dotenv('.env.local')\n",
    "    \n",
    "    supabase_url = os.environ.get(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "    supabase_key = os.environ.get(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "\n",
    "    if not supabase_url or not supabase_key:\n",
    "        print(\"Error: Supabase URL or Key not found in .env.local file.\")\n",
    "        print(\"Please create a .env.local file with your credentials.\")\n",
    "        return\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize Supabase client\n",
    "    supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "    # --- Data Fetching and Processing ---\n",
    "    raw_df = fetch_analytics_data(supabase)\n",
    "    if raw_df.empty:\n",
    "        print(\"Exiting script as no data was fetched.\")\n",
    "        return\n",
    "        \n",
    "    all_learners_data = process_learner_data(raw_df)\n",
    "    \n",
    "    if not all_learners_data:\n",
    "        print(\"Exiting script as no learner data could be processed.\")\n",
    "        return\n",
    "\n",
    "    # --- Report Generation ---\n",
    "    print(\"\\nGenerating individual learner reports...\")\n",
    "    for user_id in all_learners_data:\n",
    "        generate_individual_report(all_learners_data[user_id], OUTPUT_DIR)\n",
    "    \n",
    "    print(\"\\nGenerating master report...\")\n",
    "    generate_master_report(all_learners_data, OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"\\nâœ… All reports have been generated and saved in the '{OUTPUT_DIR}' folder.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting report generation script...\n",
      "Fetching data for learners starting with 'BT'...\n",
      "Successfully fetched 107 records.\n",
      "Processing raw data...\n",
      "  -> Processing data for BT8BT9 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT15BT16 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT14 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT2 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT4 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT13 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT7 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT1 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT11BT12 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT7 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT5 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT3+BT6 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT10 on Goal: 'Break Even and Build Trust'\n",
      "Data processing complete.\n",
      "Calculating aggregate statistics...\n",
      "\n",
      "Generating individual learner reports...\n",
      "  -> Generating report for BT8BT9 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT15BT16 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT14 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT2 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT4 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT13 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT7 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT1 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT11BT12 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT7 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT5 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT3+BT6 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT10 (Goal: Break Even and Build Trust)...\n",
      "\n",
      "Generating master report...\n",
      "-> Generating Master Report...\n",
      "   Master Report saved.\n",
      "\n",
      "âœ… All reports have been generated and saved in the 'GeneratedReports' folder.\n"
     ]
    }
   ],
   "source": [
    "# generate_reports.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data and DB Libraries\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Reporting and Plotting Libraries\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = Path(\"GeneratedReports\")\n",
    "LEARNER_PREFIX = \"BT\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def add_df_to_doc(doc, df, title=\"\"):\n",
    "    \"\"\"Adds a rounded Pandas DataFrame to a docx document with a title.\"\"\"\n",
    "    if title:\n",
    "        doc.add_heading(title, level=2)\n",
    "    \n",
    "    if df.empty:\n",
    "        doc.add_paragraph(\"No data available.\")\n",
    "        return\n",
    "        \n",
    "    # Round all numeric columns to 2 decimal places\n",
    "    df_rounded = df.copy()\n",
    "    for col in df_rounded.select_dtypes(include=['number']).columns:\n",
    "        df_rounded[col] = df_rounded[col].round(2)\n",
    "\n",
    "    # Reset index if it's a simple range, otherwise keep it as a column\n",
    "    if isinstance(df_rounded.index, pd.RangeIndex):\n",
    "        df_for_table = df_rounded.reset_index(drop=True)\n",
    "    else:\n",
    "        df_for_table = df_rounded.reset_index()\n",
    "\n",
    "    table = doc.add_table(rows=1, cols=len(df_for_table.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    \n",
    "    for j, col_name in enumerate(df_for_table.columns):\n",
    "        table.cell(0, j).text = str(col_name)\n",
    "\n",
    "    for i, row in df_for_table.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, val in enumerate(row):\n",
    "            row_cells[j].text = str(val)\n",
    "\n",
    "def create_plot_from_df(df, title, y_label, x_col='Decision Point'):\n",
    "    \"\"\"Creates a line plot from a dataframe and returns it as an in-memory image.\"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5.5))\n",
    "    \n",
    "    # Plotting logic\n",
    "    for col in df.columns:\n",
    "        if col != x_col:\n",
    "            ax.plot(df[x_col], df[col], marker='o', linestyle='-', label=col)\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_col, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    \n",
    "    # Adjust legend and layout\n",
    "    if len(df.columns) > 2: # More than one line to plot\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        fig.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "        \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=300)\n",
    "    plt.close(fig)\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "# --- CORE SCRIPT LOGIC ---\n",
    "\n",
    "def fetch_analytics_data(supabase: Client) -> pd.DataFrame:\n",
    "    \"\"\"Fetches and cleans data, including the goal name.\"\"\"\n",
    "    print(f\"Fetching data for learners starting with '{LEARNER_PREFIX}'...\")\n",
    "    try:\n",
    "        # UPDATED QUERY: Joins with goals table to get goal name\n",
    "        response = supabase.table('historical_learning_analytics').select(\n",
    "            'created_at, user_id, goal_id, scenario_attempt_number, decision_number, '\n",
    "            'kc_scores_after_decision, metric_values_after_decision, '\n",
    "            'users!inner(name), goals!inner(name)'\n",
    "        ).like(\n",
    "            'users.name', f'{LEARNER_PREFIX}%'\n",
    "        ).order(\n",
    "            'user_id'\n",
    "        ).order(\n",
    "            'created_at', desc=False\n",
    "        ).execute()\n",
    "\n",
    "        if not response.data:\n",
    "            print(\"No data found for the specified learners.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        df = pd.DataFrame(response.data)\n",
    "        \n",
    "        # Unpack nested data\n",
    "        df['user_name'] = df['users'].apply(lambda x: x['name'])\n",
    "        df['goal_name'] = df['goals'].apply(lambda x: x['name'])\n",
    "        df = df.drop(columns=['users', 'goals'])\n",
    "        \n",
    "        # Convert created_at to datetime objects for proper sorting\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        \n",
    "        print(f\"Successfully fetched {len(df)} records.\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_learner_data(raw_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Processes raw data, adds a 'Genesis' point, and structures it by learner.\"\"\"\n",
    "    print(\"Processing raw data...\")\n",
    "    if raw_df.empty:\n",
    "        return {}\n",
    "\n",
    "    all_learners_data = {}\n",
    "    \n",
    "    # Get all unique KC and Metric names across the entire dataset\n",
    "    all_kcs = set()\n",
    "    all_metrics = set()\n",
    "    raw_df['kc_scores_after_decision'].dropna().apply(lambda x: all_kcs.update(x.keys()))\n",
    "    raw_df['metric_values_after_decision'].dropna().apply(lambda x: all_metrics.update(x.keys()))\n",
    "    all_kcs, all_metrics = sorted(list(all_kcs)), sorted(list(all_metrics))\n",
    "\n",
    "    for (user_id, goal_id), group in raw_df.groupby(['user_id', 'goal_id']):\n",
    "        user_name = group['user_name'].iloc[0]\n",
    "        goal_name = group['goal_name'].iloc[0]\n",
    "        \n",
    "        print(f\"  -> Processing data for {user_name} on Goal: '{goal_name}'\")\n",
    "\n",
    "        learner_df = group.sort_values('created_at').reset_index(drop=True)\n",
    "        \n",
    "        # Expand JSON columns\n",
    "        kc_df = pd.json_normalize(learner_df['kc_scores_after_decision']).reindex(columns=all_kcs).fillna(0)\n",
    "        metric_df = pd.json_normalize(learner_df['metric_values_after_decision']).reindex(columns=all_metrics).fillna(0)\n",
    "        \n",
    "        # Create a clean 'Decision Point' identifier\n",
    "        learner_df['Decision Point'] = [f\"G{r.goal_id}-S{r.scenario_attempt_number}-D{r.decision_number}\" for r in learner_df.itertuples()]\n",
    "\n",
    "        # --- ADD GENESIS POINT ---\n",
    "        genesis_row = {\n",
    "            'created_at': learner_df['created_at'].iloc[0] - timedelta(seconds=1),\n",
    "            'Decision Point': 'Genesis',\n",
    "            **{kc: 0 for kc in all_kcs},\n",
    "            **{metric: 0 for metric in all_metrics}\n",
    "        }\n",
    "        genesis_df = pd.DataFrame([genesis_row])\n",
    "        \n",
    "        # Combine everything into a clean DataFrame for the learner\n",
    "        base_info = learner_df[['created_at', 'Decision Point']]\n",
    "        processed_df = pd.concat([base_info, kc_df, metric_df], axis=1)\n",
    "        \n",
    "        # Prepend the Genesis row\n",
    "        full_df = pd.concat([genesis_df, processed_df], ignore_index=True)\n",
    "        \n",
    "        # Identify active KCs and Metrics (ones that changed from 0)\n",
    "        final_scores = full_df.iloc[-1]\n",
    "        active_kcs = [kc for kc in all_kcs if final_scores[kc] != 0]\n",
    "        active_metrics = [metric for metric in all_metrics if final_scores[metric] != 0]\n",
    "\n",
    "        all_learners_data[f\"{user_id}_{goal_id}\"] = {\n",
    "            \"user_name\": user_name,\n",
    "            \"goal_name\": goal_name,\n",
    "            \"full_data\": full_df,\n",
    "            \"all_kcs\": all_kcs,\n",
    "            \"all_metrics\": all_metrics,\n",
    "            \"active_kcs\": active_kcs,\n",
    "            \"active_metrics\": active_metrics\n",
    "        }\n",
    "        \n",
    "    print(\"Data processing complete.\")\n",
    "    return all_learners_data\n",
    "\n",
    "def calculate_aggregate_stats(all_learners_data: dict) -> dict:\n",
    "    \"\"\"Calculates aggregate statistics (mean, min, max) across all learners.\"\"\"\n",
    "    print(\"Calculating aggregate statistics...\")\n",
    "    if not all_learners_data:\n",
    "        return {}\n",
    "\n",
    "    final_scores_list = []\n",
    "    for data in all_learners_data.values():\n",
    "        final_row = data['full_data'].iloc[-1].copy()\n",
    "        final_row['user_name'] = data['user_name']\n",
    "        final_scores_list.append(final_row)\n",
    "    \n",
    "    if not final_scores_list:\n",
    "        return {}\n",
    "\n",
    "    final_scores_df = pd.DataFrame(final_scores_list).set_index('user_name')\n",
    "    kcs = all_learners_data[list(all_learners_data.keys())[0]]['all_kcs']\n",
    "    metrics = all_learners_data[list(all_learners_data.keys())[0]]['all_metrics']\n",
    "\n",
    "    kc_stats = final_scores_df[kcs].describe().loc[['mean', 'min', 'max']]\n",
    "    metric_stats = final_scores_df[metrics].describe().loc[['mean', 'min', 'max']]\n",
    "\n",
    "    return {\"kc\": kc_stats, \"metric\": metric_stats, \"final_scores_df\": final_scores_df}\n",
    "\n",
    "def generate_individual_report(learner_data: dict, agg_stats: dict, output_dir: Path):\n",
    "    \"\"\"Generates a single, enhanced Word document report for one learner.\"\"\"\n",
    "    user_name = learner_data['user_name']\n",
    "    goal_name = learner_data['goal_name']\n",
    "    df = learner_data['full_data']\n",
    "    active_kcs = learner_data['active_kcs']\n",
    "    active_metrics = learner_data['active_metrics']\n",
    "    \n",
    "    print(f\"  -> Generating report for {user_name} (Goal: {goal_name})...\")\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Learner Report: {user_name}', 0)\n",
    "    \n",
    "    # --- THIS IS THE CORRECTED LINE ---\n",
    "    # Use 'Quote' which is a built-in paragraph style, often italicized.\n",
    "    doc.add_paragraph(f\"Goal Attempted: {goal_name}\", style='Quote') \n",
    "    # --- END OF CORRECTION ---\n",
    "\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # --- Section 1: Final Scores with Comparison ---\n",
    "    doc.add_heading('Final Cumulative Scores', level=1)\n",
    "    doc.add_paragraph(\"This section shows the learner's final score for each item compared to the class average, minimum, and maximum.\")\n",
    "    \n",
    "    final_scores = df.iloc[-1]\n",
    "\n",
    "    # Enhanced Final KC Scores Table\n",
    "    if active_kcs:\n",
    "        kc_summary = final_scores[active_kcs].to_frame(name='Your Score')\n",
    "        kc_summary = kc_summary.join(agg_stats['kc'][active_kcs].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "        kc_summary.index.name = \"Knowledge Component (KC)\"\n",
    "        add_df_to_doc(doc, kc_summary, \"Final KC Scores\")\n",
    "\n",
    "    # Enhanced Final Metric Scores Table\n",
    "    if active_metrics:\n",
    "        metric_summary = final_scores[active_metrics].to_frame(name='Your Value')\n",
    "        metric_summary = metric_summary.join(agg_stats['metric'][active_metrics].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "        metric_summary.index.name = \"Metric\"\n",
    "        add_df_to_doc(doc, metric_summary, \"Final Metric Values\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "\n",
    "    # --- Section 2: Scores Over Time ---\n",
    "    doc.add_heading('Performance Over Time', level=1)\n",
    "    \n",
    "    # KC Scores Progression\n",
    "    if active_kcs:\n",
    "        kc_progression_df = df[['Decision Point'] + active_kcs]\n",
    "        add_df_to_doc(doc, kc_progression_df.set_index('Decision Point'), \"KC Scores per Decision Point\")\n",
    "        kc_plot_img = create_plot_from_df(kc_progression_df, \"KC Scores Over Time\", \"KC Score\")\n",
    "        doc.add_picture(kc_plot_img, width=Inches(6.5))\n",
    "    \n",
    "    # Metric Scores Progression (SPLIT PLOTS)\n",
    "    if active_metrics:\n",
    "        metric_progression_df = df[['Decision Point'] + active_metrics]\n",
    "        add_df_to_doc(doc, metric_progression_df.set_index('Decision Point'), \"Metric Values per Decision Point\")\n",
    "\n",
    "        # Revenue Plot\n",
    "        if 'Revenue' in active_metrics:\n",
    "            revenue_df = df[['Decision Point', 'Revenue']]\n",
    "            revenue_plot_img = create_plot_from_df(revenue_df, \"Revenue Over Time\", \"Revenue Value\")\n",
    "            doc.add_heading(\"Revenue Progression\", level=3)\n",
    "            doc.add_picture(revenue_plot_img, width=Inches(6.5))\n",
    "\n",
    "        # Other Metrics Plot\n",
    "        other_metrics = [m for m in active_metrics if m != 'Revenue']\n",
    "        if other_metrics:\n",
    "            other_metrics_df = df[['Decision Point'] + other_metrics]\n",
    "            other_metrics_plot_img = create_plot_from_df(other_metrics_df, \"Other Metric Values Over Time\", \"Metric Value\")\n",
    "            doc.add_heading(\"Other Metrics Progression\", level=3)\n",
    "            doc.add_picture(other_metrics_plot_img, width=Inches(6.5))\n",
    "\n",
    "    file_path = output_dir / f\"Report_{user_name.replace(' ', '_')}_{goal_name.replace(' ', '_')}.docx\"\n",
    "    doc.save(file_path)\n",
    "\n",
    "def generate_master_report(agg_stats: dict, all_kcs: list, all_metrics: list, num_learners: int, output_dir: Path):\n",
    "    \"\"\"Generates the master report with aggregate stats and leaderboards.\"\"\"\n",
    "    print(\"-> Generating Master Report...\")\n",
    "    if not agg_stats:\n",
    "        print(\"   No data available to generate a master report.\")\n",
    "        return\n",
    "\n",
    "    doc = Document()\n",
    "    doc.add_heading('Master Learner Report', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(f\"This report covers {num_learners} learner-goal attempts for names starting with '{LEARNER_PREFIX}'.\")\n",
    "\n",
    "    # --- Section 1: Aggregate Statistics ---\n",
    "    doc.add_heading('Aggregate Statistics (Final Scores)', level=1)\n",
    "    add_df_to_doc(doc, agg_stats['kc'].T, \"KC Score Statistics (Across All Learners)\")\n",
    "    add_df_to_doc(doc, agg_stats['metric'].T, \"Metric Value Statistics (Across All Learners)\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "    \n",
    "    # --- Section 2: Leaderboards ---\n",
    "    doc.add_heading('Leaderboards', level=1)\n",
    "    final_scores_df = agg_stats['final_scores_df']\n",
    "    \n",
    "    # KC Leaderboard\n",
    "    final_scores_df['Total KC Score'] = final_scores_df[all_kcs].sum(axis=1)\n",
    "    kc_leaderboard = final_scores_df[['Total KC Score']].sort_values('Total KC Score', ascending=False)\n",
    "    add_df_to_doc(doc, kc_leaderboard.head(10), \"Top 10 - KC Leaderboard (by Sum of Final KC Scores)\")\n",
    "    \n",
    "    # Metric Leaderboard\n",
    "    try:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_metrics = scaler.fit_transform(final_scores_df[all_metrics])\n",
    "        final_scores_df['Composite Metric Score'] = pd.DataFrame(normalized_metrics, index=final_scores_df.index).sum(axis=1)\n",
    "        metric_leaderboard = final_scores_df[['Composite Metric Score']].sort_values('Composite Metric Score', ascending=False)\n",
    "        \n",
    "        doc.add_paragraph(\n",
    "            \"The Metric Leaderboard is calculated by normalizing each metric (scaling from 0 to 1 based on the min/max in this group) \"\n",
    "            \"and then summing these normalized scores. This gives a balanced view of overall performance across all metrics.\"\n",
    "        )\n",
    "        add_df_to_doc(doc, metric_leaderboard.head(10), \"Top 10 - Metric Leaderboard (by Composite Score)\")\n",
    "    except ImportError:\n",
    "        doc.add_paragraph(\"Metric Leaderboard could not be generated. Please install scikit-learn: pip install scikit-learn\")\n",
    "\n",
    "    doc.save(output_dir / \"Master_Report.docx\")\n",
    "    print(\"   Master Report saved.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire reporting script.\"\"\"\n",
    "    print(\"Starting report generation script...\")\n",
    "    load_dotenv('.env.local')\n",
    "    \n",
    "    supabase_url = os.environ.get(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "    supabase_key = os.environ.get(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "\n",
    "    if not supabase_url or not supabase_key:\n",
    "        print(\"Error: Supabase URL or Key not found. Please check your .env.local file.\")\n",
    "        return\n",
    "\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "    raw_df = fetch_analytics_data(supabase)\n",
    "    if raw_df.empty:\n",
    "        print(\"Exiting script as no data was fetched.\")\n",
    "        return\n",
    "        \n",
    "    all_learners_data = process_learner_data(raw_df)\n",
    "    if not all_learners_data:\n",
    "        print(\"Exiting script as no learner data could be processed.\")\n",
    "        return\n",
    "\n",
    "    # Calculate aggregate stats once\n",
    "    agg_stats = calculate_aggregate_stats(all_learners_data)\n",
    "\n",
    "    # Generate individual reports\n",
    "    print(\"\\nGenerating individual learner reports...\")\n",
    "    for data in all_learners_data.values():\n",
    "        generate_individual_report(data, agg_stats, OUTPUT_DIR)\n",
    "    \n",
    "    # Generate master report\n",
    "    print(\"\\nGenerating master report...\")\n",
    "    first_learner = all_learners_data[list(all_learners_data.keys())[0]]\n",
    "    generate_master_report(\n",
    "        agg_stats, \n",
    "        first_learner['all_kcs'], \n",
    "        first_learner['all_metrics'], \n",
    "        len(all_learners_data), \n",
    "        OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… All reports have been generated and saved in the '{OUTPUT_DIR}' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting report generation script...\n",
      "Fetching data for learners starting with 'BT'...\n",
      "Successfully fetched 107 records.\n",
      "Processing raw data...\n",
      "  -> Processing data for BT8BT9 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT15BT16 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT14 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT2 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT4 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT13 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT7 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT1 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT11BT12 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT7 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT5 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT3+BT6 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT10 on Goal: 'Break Even and Build Trust'\n",
      "Data processing complete.\n",
      "Calculating statistics per goal...\n",
      "  -> Calculating stats for goal: 'Break Even and Build Trust'\n",
      "  -> Calculating stats for goal: 'Price with Purpose'\n",
      "\n",
      "Generating individual learner reports...\n",
      "  -> Generating report for BT8BT9 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT15BT16 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT14 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT2 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT4 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT13 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT7 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT1 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT11BT12 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT7 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT5 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT3+BT6 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT10 (Goal: Break Even and Build Trust)...\n",
      "\n",
      "Generating master report...\n",
      "-> Generating Master Report...\n",
      "   Master Report saved.\n",
      "\n",
      "âœ… All reports have been generated and saved in the 'GeneratedReports' folder.\n"
     ]
    }
   ],
   "source": [
    "# generate_reports.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data and DB Libraries\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Reporting and Plotting Libraries\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = Path(\"GeneratedReports\")\n",
    "LEARNER_PREFIX = \"BT\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def add_df_to_doc(doc, df, title=\"\"):\n",
    "    # (This function remains the same as the previous version)\n",
    "    if title:\n",
    "        doc.add_heading(title, level=2)\n",
    "    if df.empty:\n",
    "        doc.add_paragraph(\"No data available.\")\n",
    "        return\n",
    "    df_rounded = df.copy()\n",
    "    for col in df_rounded.select_dtypes(include=['number']).columns:\n",
    "        df_rounded[col] = df_rounded[col].round(2)\n",
    "    if isinstance(df_rounded.index, pd.RangeIndex):\n",
    "        df_for_table = df_rounded.reset_index(drop=True)\n",
    "    else:\n",
    "        df_for_table = df_rounded.reset_index()\n",
    "    table = doc.add_table(rows=1, cols=len(df_for_table.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    for j, col_name in enumerate(df_for_table.columns):\n",
    "        table.cell(0, j).text = str(col_name)\n",
    "    for i, row in df_for_table.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, val in enumerate(row):\n",
    "            row_cells[j].text = str(val)\n",
    "\n",
    "def create_plot_from_df(df, title, y_label, x_col='Decision Point'):\n",
    "    # (This function remains the same as the previous version)\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5.5))\n",
    "    for col in df.columns:\n",
    "        if col != x_col:\n",
    "            ax.plot(df[x_col], df[col], marker='o', linestyle='-', label=col)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_col, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    if len(df.columns) > 2:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        fig.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=300)\n",
    "    plt.close(fig)\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "# --- CORE SCRIPT LOGIC ---\n",
    "\n",
    "def fetch_analytics_data(supabase: Client) -> pd.DataFrame:\n",
    "    # (This function remains the same as the previous version)\n",
    "    print(f\"Fetching data for learners starting with '{LEARNER_PREFIX}'...\")\n",
    "    try:\n",
    "        response = supabase.table('historical_learning_analytics').select(\n",
    "            'created_at, user_id, goal_id, scenario_attempt_number, decision_number, '\n",
    "            'kc_scores_after_decision, metric_values_after_decision, '\n",
    "            'users!inner(name), goals!inner(name)'\n",
    "        ).like('users.name', f'{LEARNER_PREFIX}%').order('user_id').order('created_at', desc=False).execute()\n",
    "        if not response.data:\n",
    "            print(\"No data found for the specified learners.\")\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame(response.data)\n",
    "        df['user_name'] = df['users'].apply(lambda x: x['name'])\n",
    "        df['goal_name'] = df['goals'].apply(lambda x: x['name'])\n",
    "        df = df.drop(columns=['users', 'goals'])\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        print(f\"Successfully fetched {len(df)} records.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_learner_data(raw_df: pd.DataFrame) -> dict:\n",
    "    # (This function remains the same as the previous version)\n",
    "    print(\"Processing raw data...\")\n",
    "    if raw_df.empty: return {}\n",
    "    all_learners_data = {}\n",
    "    all_kcs = set()\n",
    "    all_metrics = set()\n",
    "    raw_df['kc_scores_after_decision'].dropna().apply(lambda x: all_kcs.update(x.keys()))\n",
    "    raw_df['metric_values_after_decision'].dropna().apply(lambda x: all_metrics.update(x.keys()))\n",
    "    all_kcs, all_metrics = sorted(list(all_kcs)), sorted(list(all_metrics))\n",
    "    for (user_id, goal_id), group in raw_df.groupby(['user_id', 'goal_id']):\n",
    "        user_name = group['user_name'].iloc[0]\n",
    "        goal_name = group['goal_name'].iloc[0]\n",
    "        print(f\"  -> Processing data for {user_name} on Goal: '{goal_name}'\")\n",
    "        learner_df = group.sort_values('created_at').reset_index(drop=True)\n",
    "        kc_df = pd.json_normalize(learner_df['kc_scores_after_decision']).reindex(columns=all_kcs).fillna(0)\n",
    "        metric_df = pd.json_normalize(learner_df['metric_values_after_decision']).reindex(columns=all_metrics).fillna(0)\n",
    "        learner_df['Decision Point'] = [f\"G{r.goal_id}-S{r.scenario_attempt_number}-D{r.decision_number}\" for r in learner_df.itertuples()]\n",
    "        genesis_row = {'created_at': learner_df['created_at'].iloc[0] - timedelta(seconds=1), 'Decision Point': 'Genesis', **{kc: 0 for kc in all_kcs}, **{metric: 0 for metric in all_metrics}}\n",
    "        genesis_df = pd.DataFrame([genesis_row])\n",
    "        processed_df = pd.concat([learner_df[['created_at', 'Decision Point']], kc_df, metric_df], axis=1)\n",
    "        full_df = pd.concat([genesis_df, processed_df], ignore_index=True)\n",
    "        final_scores = full_df.iloc[-1]\n",
    "        active_kcs = [kc for kc in all_kcs if final_scores[kc] != 0]\n",
    "        active_metrics = [metric for metric in all_metrics if final_scores[metric] != 0]\n",
    "        all_learners_data[f\"{user_id}_{goal_id}\"] = {\"user_name\": user_name, \"goal_name\": goal_name, \"full_data\": full_df, \"all_kcs\": all_kcs, \"all_metrics\": all_metrics, \"active_kcs\": active_kcs, \"active_metrics\": active_metrics}\n",
    "    print(\"Data processing complete.\")\n",
    "    return all_learners_data\n",
    "\n",
    "# --- MODIFIED FUNCTION ---\n",
    "def calculate_all_stats(all_learners_data: dict):\n",
    "    \"\"\"Calculates statistics PER GOAL and returns a combined final scores DataFrame.\"\"\"\n",
    "    print(\"Calculating statistics per goal...\")\n",
    "    if not all_learners_data:\n",
    "        return {}, pd.DataFrame()\n",
    "\n",
    "    final_scores_list = []\n",
    "    for data in all_learners_data.values():\n",
    "        final_row = data['full_data'].iloc[-1].copy()\n",
    "        final_row['user_name'] = data['user_name']\n",
    "        final_row['goal_name'] = data['goal_name'] # Keep goal name for grouping\n",
    "        final_scores_list.append(final_row)\n",
    "    \n",
    "    final_scores_df = pd.DataFrame(final_scores_list)\n",
    "    all_kcs = all_learners_data[list(all_learners_data.keys())[0]]['all_kcs']\n",
    "    all_metrics = all_learners_data[list(all_learners_data.keys())[0]]['all_metrics']\n",
    "\n",
    "    per_goal_stats = {}\n",
    "    for goal_name, group_df in final_scores_df.groupby('goal_name'):\n",
    "        print(f\"  -> Calculating stats for goal: '{goal_name}'\")\n",
    "        kc_stats = group_df[all_kcs].describe().loc[['mean', 'min', 'max']]\n",
    "        metric_stats = group_df[all_metrics].describe().loc[['mean', 'min', 'max']]\n",
    "        per_goal_stats[goal_name] = {'kc': kc_stats, 'metric': metric_stats}\n",
    "        \n",
    "    return per_goal_stats, final_scores_df\n",
    "\n",
    "# --- MODIFIED FUNCTION ---\n",
    "def generate_individual_report(learner_data: dict, per_goal_stats: dict, output_dir: Path):\n",
    "    \"\"\"Generates an individual report with goal-specific stats and split metric plots.\"\"\"\n",
    "    user_name = learner_data['user_name']\n",
    "    goal_name = learner_data['goal_name']\n",
    "    df = learner_data['full_data']\n",
    "    active_kcs = learner_data['active_kcs']\n",
    "    active_metrics = learner_data['active_metrics']\n",
    "\n",
    "    print(f\"  -> Generating report for {user_name} (Goal: {goal_name})...\")\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Learner Report: {user_name}', 0)\n",
    "    doc.add_paragraph(f\"Goal Attempted: {goal_name}\", style='Quote')\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    doc.add_heading('Final Cumulative Scores', level=1)\n",
    "    doc.add_paragraph(\n",
    "        \"This section shows the learner's final score compared to the average, min, and max \"\n",
    "        \"of all learners who attempted THIS SAME GOAL.\"\n",
    "    )\n",
    "    \n",
    "    final_scores = df.iloc[-1]\n",
    "    # Get stats for the specific goal this learner attempted\n",
    "    specific_goal_stats = per_goal_stats.get(goal_name, {})\n",
    "\n",
    "    if specific_goal_stats:\n",
    "        # Final KC Scores Table\n",
    "        if active_kcs:\n",
    "            kc_summary = final_scores[active_kcs].to_frame(name='Your Score')\n",
    "            kc_summary = kc_summary.join(specific_goal_stats['kc'][active_kcs].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "            add_df_to_doc(doc, kc_summary, \"Final KC Scores\")\n",
    "\n",
    "        # Final Metric Scores Table\n",
    "        if active_metrics:\n",
    "            metric_summary = final_scores[active_metrics].to_frame(name='Your Value')\n",
    "            metric_summary = metric_summary.join(specific_goal_stats['metric'][active_metrics].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "            add_df_to_doc(doc, metric_summary, \"Final Metric Values\")\n",
    "\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Performance Over Time', level=1)\n",
    "    \n",
    "    # KC Scores Progression\n",
    "    if active_kcs:\n",
    "        kc_progression_df = df[['Decision Point'] + active_kcs]\n",
    "        add_df_to_doc(doc, kc_progression_df.set_index('Decision Point'), \"KC Scores per Decision Point\")\n",
    "        kc_plot_img = create_plot_from_df(kc_progression_df, \"KC Scores Over Time\", \"KC Score\")\n",
    "        doc.add_picture(kc_plot_img, width=Inches(6.5))\n",
    "    \n",
    "    # --- NEW SPLIT METRIC PLOTTING LOGIC ---\n",
    "    if active_metrics:\n",
    "        metric_progression_df = df[['Decision Point'] + active_metrics]\n",
    "        add_df_to_doc(doc, metric_progression_df.set_index('Decision Point'), \"Metric Values per Decision Point\")\n",
    "\n",
    "        # Plot 1: Revenue\n",
    "        if 'Revenue' in active_metrics:\n",
    "            revenue_df = df[['Decision Point', 'Revenue']]\n",
    "            revenue_plot_img = create_plot_from_df(revenue_df, \"Revenue Over Time\", \"Revenue Value\")\n",
    "            doc.add_heading(\"Revenue Progression\", level=3)\n",
    "            doc.add_picture(revenue_plot_img, width=Inches(6.5))\n",
    "\n",
    "        # Plot 2: Reputation\n",
    "        if 'Reputation' in active_metrics:\n",
    "            reputation_df = df[['Decision Point', 'Reputation']]\n",
    "            rep_plot_img = create_plot_from_df(reputation_df, \"Reputation Over Time\", \"Reputation (1-5 scale)\")\n",
    "            doc.add_heading(\"Reputation Progression\", level=3)\n",
    "            doc.add_picture(rep_plot_img, width=Inches(6.5))\n",
    "\n",
    "        # Plot 3: Other high-scale metrics\n",
    "        other_metrics = [m for m in ['Customer Satisfaction', 'Risk-Taking', 'Ethical Decision Making'] if m in active_metrics]\n",
    "        if other_metrics:\n",
    "            other_metrics_df = df[['Decision Point'] + other_metrics]\n",
    "            other_plot_img = create_plot_from_df(other_metrics_df, \"Other Metric Values Over Time\", \"Metric Value (0-100 scale)\")\n",
    "            doc.add_heading(\"High-Scale Metrics Progression\", level=3)\n",
    "            doc.add_picture(other_plot_img, width=Inches(6.5))\n",
    "\n",
    "    file_path = output_dir / f\"Report_{user_name.replace(' ', '_')}_{goal_name.replace(' ', '_')}.docx\"\n",
    "    doc.save(file_path)\n",
    "\n",
    "# --- MODIFIED FUNCTION ---\n",
    "def generate_master_report(final_scores_df: pd.DataFrame, all_kcs: list, all_metrics: list, num_learners: int, output_dir: Path):\n",
    "    \"\"\"Generates the master report with global stats and leaderboards.\"\"\"\n",
    "    print(\"-> Generating Master Report...\")\n",
    "    if final_scores_df.empty:\n",
    "        print(\"   No data available to generate a master report.\")\n",
    "        return\n",
    "\n",
    "    doc = Document()\n",
    "    doc.add_heading('Master Learner Report', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(f\"This report covers {num_learners} learner-goal attempts for names starting with '{LEARNER_PREFIX}'.\")\n",
    "\n",
    "    # Aggregate stats are now calculated here for the global view\n",
    "    doc.add_heading('Global Aggregate Statistics (Final Scores)', level=1)\n",
    "    kc_stats = final_scores_df[all_kcs].describe().loc[['mean', 'min', 'max']]\n",
    "    metric_stats = final_scores_df[all_metrics].describe().loc[['mean', 'min', 'max']]\n",
    "    add_df_to_doc(doc, kc_stats.T, \"Global KC Score Statistics\")\n",
    "    add_df_to_doc(doc, metric_stats.T, \"Global Metric Value Statistics\")\n",
    "    \n",
    "    doc.add_page_break()\n",
    "    \n",
    "    doc.add_heading('Leaderboards', level=1)\n",
    "    final_scores_df = final_scores_df.set_index('user_name')\n",
    "    \n",
    "    # KC Leaderboard\n",
    "    final_scores_df['Total KC Score'] = final_scores_df[all_kcs].sum(axis=1)\n",
    "    add_df_to_doc(doc, final_scores_df[['Total KC Score']].sort_values('Total KC Score', ascending=False).head(10), \"Top 10 - KC Leaderboard\")\n",
    "    \n",
    "    # Metric Leaderboard\n",
    "    try:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_metrics = scaler.fit_transform(final_scores_df[all_metrics])\n",
    "        final_scores_df['Composite Metric Score'] = pd.DataFrame(normalized_metrics, index=final_scores_df.index).sum(axis=1)\n",
    "        doc.add_paragraph(\"The Metric Leaderboard is calculated by normalizing each metric globally and summing these scores.\")\n",
    "        add_df_to_doc(doc, final_scores_df[['Composite Metric Score']].sort_values('Composite Metric Score', ascending=False).head(10), \"Top 10 - Metric Leaderboard\")\n",
    "    except ImportError:\n",
    "        doc.add_paragraph(\"Metric Leaderboard requires scikit-learn: pip install scikit-learn\")\n",
    "\n",
    "    doc.save(output_dir / \"Master_Report.docx\")\n",
    "    print(\"   Master Report saved.\")\n",
    "\n",
    "# --- MODIFIED MAIN FUNCTION ---\n",
    "def main():\n",
    "    \"\"\"Main function to run the entire reporting script.\"\"\"\n",
    "    print(\"Starting report generation script...\")\n",
    "    load_dotenv('.env.local')\n",
    "    \n",
    "    supabase_url = os.environ.get(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "    supabase_key = os.environ.get(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "\n",
    "    if not supabase_url or not supabase_key:\n",
    "        print(\"Error: Supabase credentials not found. Check .env.local file.\")\n",
    "        return\n",
    "\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "    raw_df = fetch_analytics_data(supabase)\n",
    "    if raw_df.empty: return\n",
    "        \n",
    "    all_learners_data = process_learner_data(raw_df)\n",
    "    if not all_learners_data: return\n",
    "\n",
    "    # Calculate stats (per goal and combined)\n",
    "    per_goal_stats, final_scores_df = calculate_all_stats(all_learners_data)\n",
    "\n",
    "    # Generate individual reports\n",
    "    print(\"\\nGenerating individual learner reports...\")\n",
    "    for data in all_learners_data.values():\n",
    "        generate_individual_report(data, per_goal_stats, OUTPUT_DIR)\n",
    "    \n",
    "    # Generate master report\n",
    "    print(\"\\nGenerating master report...\")\n",
    "    first_learner = all_learners_data[list(all_learners_data.keys())[0]]\n",
    "    generate_master_report(\n",
    "        final_scores_df, \n",
    "        first_learner['all_kcs'], \n",
    "        first_learner['all_metrics'], \n",
    "        len(all_learners_data), \n",
    "        OUTPUT_DIR\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… All reports have been generated and saved in the '{OUTPUT_DIR}' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_reports.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data and DB Libraries\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Reporting and Plotting Libraries\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = Path(\"GeneratedReports\")\n",
    "LEARNER_PREFIX = \"BT\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def add_df_to_doc(doc, df, title=\"\"):\n",
    "    if title:\n",
    "        doc.add_heading(title, level=2)\n",
    "    if df.empty:\n",
    "        doc.add_paragraph(\"No data available.\")\n",
    "        return\n",
    "    df_rounded = df.copy()\n",
    "    for col in df_rounded.select_dtypes(include=['number']).columns:\n",
    "        df_rounded[col] = df_rounded[col].round(2)\n",
    "    if isinstance(df_rounded.index, pd.RangeIndex):\n",
    "        df_for_table = df_rounded.reset_index(drop=True)\n",
    "    else:\n",
    "        df_for_table = df_rounded.reset_index()\n",
    "    table = doc.add_table(rows=1, cols=len(df_for_table.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    for j, col_name in enumerate(df_for_table.columns):\n",
    "        table.cell(0, j).text = str(col_name)\n",
    "    for i, row in df_for_table.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, val in enumerate(row):\n",
    "            row_cells[j].text = str(val)\n",
    "\n",
    "def create_plot_from_df(df, title, y_label, x_col='Decision Point'):\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5.5))\n",
    "    for col in df.columns:\n",
    "        if col != x_col:\n",
    "            ax.plot(df[x_col], df[col], marker='o', linestyle='-', label=col)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_col, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    # Add a legend only if there is more than one data series to plot\n",
    "    if len(df.columns) > 2:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        fig.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=300)\n",
    "    plt.close(fig)\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "# --- CORE SCRIPT LOGIC ---\n",
    "\n",
    "def fetch_analytics_data(supabase: Client) -> pd.DataFrame:\n",
    "    print(f\"Fetching data for learners starting with '{LEARNER_PREFIX}'...\")\n",
    "    try:\n",
    "        response = supabase.table('historical_learning_analytics').select(\n",
    "            'created_at, user_id, goal_id, scenario_attempt_number, decision_number, '\n",
    "            'kc_scores_after_decision, metric_values_after_decision, '\n",
    "            'users!inner(name), goals!inner(name)'\n",
    "        ).like('users.name', f'{LEARNER_PREFIX}%').order('user_id').order('created_at', desc=False).execute()\n",
    "        if not response.data:\n",
    "            print(\"No data found for the specified learners.\")\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame(response.data)\n",
    "        df['user_name'] = df['users'].apply(lambda x: x['name'])\n",
    "        df['goal_name'] = df['goals'].apply(lambda x: x['name'])\n",
    "        df = df.drop(columns=['users', 'goals'])\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        print(f\"Successfully fetched {len(df)} records.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_learner_data(raw_df: pd.DataFrame) -> dict:\n",
    "    print(\"Processing raw data...\")\n",
    "    if raw_df.empty: return {}\n",
    "    all_learners_data = {}\n",
    "    all_kcs = set()\n",
    "    all_metrics = set()\n",
    "    raw_df['kc_scores_after_decision'].dropna().apply(lambda x: all_kcs.update(x.keys()))\n",
    "    raw_df['metric_values_after_decision'].dropna().apply(lambda x: all_metrics.update(x.keys()))\n",
    "    all_kcs, all_metrics = sorted(list(all_kcs)), sorted(list(all_metrics))\n",
    "    for (user_id, goal_id), group in raw_df.groupby(['user_id', 'goal_id']):\n",
    "        user_name = group['user_name'].iloc[0]\n",
    "        goal_name = group['goal_name'].iloc[0]\n",
    "        print(f\"  -> Processing data for {user_name} on Goal: '{goal_name}'\")\n",
    "        learner_df = group.sort_values('created_at').reset_index(drop=True)\n",
    "        kc_df = pd.json_normalize(learner_df['kc_scores_after_decision']).reindex(columns=all_kcs).fillna(0)\n",
    "        metric_df = pd.json_normalize(learner_df['metric_values_after_decision']).reindex(columns=all_metrics).fillna(0)\n",
    "        learner_df['Decision Point'] = [f\"G{r.goal_id}-S{r.scenario_attempt_number}-D{r.decision_number}\" for r in learner_df.itertuples()]\n",
    "        genesis_row = {'created_at': learner_df['created_at'].iloc[0] - timedelta(seconds=1), 'Decision Point': 'Genesis', **{kc: 0 for kc in all_kcs}, **{metric: 0 for metric in all_metrics}}\n",
    "        genesis_df = pd.DataFrame([genesis_row])\n",
    "        processed_df = pd.concat([learner_df[['created_at', 'Decision Point']], kc_df, metric_df], axis=1)\n",
    "        full_df = pd.concat([genesis_df, processed_df], ignore_index=True)\n",
    "        final_scores = full_df.iloc[-1]\n",
    "        active_kcs = [kc for kc in all_kcs if final_scores[kc] != 0]\n",
    "        active_metrics = [metric for metric in all_metrics if final_scores[metric] != 0]\n",
    "        all_learners_data[f\"{user_id}_{goal_id}\"] = {\"user_name\": user_name, \"goal_name\": goal_name, \"full_data\": full_df, \"all_kcs\": all_kcs, \"all_metrics\": all_metrics, \"active_kcs\": active_kcs, \"active_metrics\": active_metrics}\n",
    "    print(\"Data processing complete.\")\n",
    "    return all_learners_data\n",
    "\n",
    "def calculate_all_stats(all_learners_data: dict):\n",
    "    print(\"Calculating statistics per goal...\")\n",
    "    if not all_learners_data:\n",
    "        return {}, pd.DataFrame()\n",
    "    final_scores_list = []\n",
    "    for data in all_learners_data.values():\n",
    "        final_row = data['full_data'].iloc[-1].copy()\n",
    "        final_row['user_name'] = data['user_name']\n",
    "        final_row['goal_name'] = data['goal_name']\n",
    "        final_scores_list.append(final_row)\n",
    "    final_scores_df = pd.DataFrame(final_scores_list)\n",
    "    all_kcs = all_learners_data[list(all_learners_data.keys())[0]]['all_kcs']\n",
    "    all_metrics = all_learners_data[list(all_learners_data.keys())[0]]['all_metrics']\n",
    "    per_goal_stats = {}\n",
    "    for goal_name, group_df in final_scores_df.groupby('goal_name'):\n",
    "        print(f\"  -> Calculating stats for goal: '{goal_name}'\")\n",
    "        kc_stats = group_df[all_kcs].describe().loc[['mean', 'min', 'max']]\n",
    "        metric_stats = group_df[all_metrics].describe().loc[['mean', 'min', 'max']]\n",
    "        per_goal_stats[goal_name] = {'kc': kc_stats, 'metric': metric_stats}\n",
    "    return per_goal_stats, final_scores_df\n",
    "\n",
    "def generate_individual_report(learner_data: dict, per_goal_stats: dict, output_dir: Path):\n",
    "    \"\"\"Generates an individual report with goal-specific stats and smart, split metric plots.\"\"\"\n",
    "    user_name = learner_data['user_name']\n",
    "    goal_name = learner_data['goal_name']\n",
    "    df = learner_data['full_data']\n",
    "    active_kcs = learner_data['active_kcs']\n",
    "    active_metrics = learner_data['active_metrics']\n",
    "\n",
    "    print(f\"  -> Generating report for {user_name} (Goal: {goal_name})...\")\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Learner Report: {user_name}', 0)\n",
    "    doc.add_paragraph(f\"Goal Attempted: {goal_name}\", style='Quote')\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    doc.add_heading('Final Cumulative Scores', level=1)\n",
    "    doc.add_paragraph(\n",
    "        \"This section shows the learner's final score compared to the average, min, and max \"\n",
    "        \"of all learners who attempted THIS SAME GOAL.\"\n",
    "    )\n",
    "    \n",
    "    final_scores = df.iloc[-1]\n",
    "    specific_goal_stats = per_goal_stats.get(goal_name, {})\n",
    "\n",
    "    if specific_goal_stats:\n",
    "        if active_kcs:\n",
    "            kc_summary = final_scores[active_kcs].to_frame(name='Your Score')\n",
    "            kc_summary = kc_summary.join(specific_goal_stats['kc'][active_kcs].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "            add_df_to_doc(doc, kc_summary, \"Final KC Scores\")\n",
    "        if active_metrics:\n",
    "            metric_summary = final_scores[active_metrics].to_frame(name='Your Value')\n",
    "            metric_summary = metric_summary.join(specific_goal_stats['metric'][active_metrics].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "            add_df_to_doc(doc, metric_summary, \"Final Metric Values\")\n",
    "\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Performance Over Time', level=1)\n",
    "    \n",
    "    if active_kcs:\n",
    "        kc_progression_df = df[['Decision Point'] + active_kcs]\n",
    "        add_df_to_doc(doc, kc_progression_df.set_index('Decision Point'), \"KC Scores per Decision Point\")\n",
    "        kc_plot_img = create_plot_from_df(kc_progression_df, \"KC Scores Over Time\", \"KC Score\")\n",
    "        doc.add_picture(kc_plot_img, width=Inches(6.5))\n",
    "    \n",
    "    if active_metrics:\n",
    "        metric_progression_df = df[['Decision Point'] + active_metrics]\n",
    "        add_df_to_doc(doc, metric_progression_df.set_index('Decision Point'), \"Metric Values per Decision Point\")\n",
    "\n",
    "        if 'Revenue' in active_metrics:\n",
    "            revenue_df = df[['Decision Point', 'Revenue']]\n",
    "            revenue_plot_img = create_plot_from_df(revenue_df, \"Revenue Over Time\", \"Revenue Value\")\n",
    "            doc.add_heading(\"Revenue Progression\", level=3)\n",
    "            doc.add_picture(revenue_plot_img, width=Inches(6.5))\n",
    "\n",
    "        if 'Reputation' in active_metrics:\n",
    "            reputation_df = df[['Decision Point', 'Reputation']]\n",
    "            rep_plot_img = create_plot_from_df(reputation_df, \"Reputation Over Time\", \"Reputation (1-5 scale)\")\n",
    "            doc.add_heading(\"Reputation Progression\", level=3)\n",
    "            doc.add_picture(rep_plot_img, width=Inches(6.5))\n",
    "\n",
    "        # --- FINAL REFINED PLOTTING LOGIC ---\n",
    "        other_metrics = [m for m in ['Customer Satisfaction', 'Risk-Taking', 'Ethical Decision Making'] if m in active_metrics]\n",
    "        if other_metrics:\n",
    "            other_metrics_df = df[['Decision Point'] + other_metrics]\n",
    "            \n",
    "            # If only one metric, be specific. Otherwise, be generic and use a legend.\n",
    "            if len(other_metrics) == 1:\n",
    "                metric_name = other_metrics[0]\n",
    "                plot_title = f\"{metric_name} Progression\"\n",
    "                y_label = f\"{metric_name} (0-100 scale)\"\n",
    "            else:\n",
    "                plot_title = \"High-Scale Metrics Progression\"\n",
    "                y_label = \"Metric Value (0-100 scale)\"\n",
    "            \n",
    "            doc.add_heading(plot_title, level=3)\n",
    "            other_plot_img = create_plot_from_df(other_metrics_df, plot_title, y_label)\n",
    "            doc.add_picture(other_plot_img, width=Inches(6.5))\n",
    "\n",
    "    file_path = output_dir / f\"Report_{user_name.replace(' ', '_')}_{goal_name.replace(' ', '_')}.docx\"\n",
    "    doc.save(file_path)\n",
    "\n",
    "\n",
    "def generate_master_report(final_scores_df: pd.DataFrame, all_kcs: list, all_metrics: list, num_learners: int, output_dir: Path):\n",
    "    print(\"-> Generating Master Report...\")\n",
    "    if final_scores_df.empty:\n",
    "        print(\"   No data available to generate a master report.\")\n",
    "        return\n",
    "    doc = Document()\n",
    "    doc.add_heading('Master Learner Report', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(f\"This report covers {num_learners} learner-goal attempts for names starting with '{LEARNER_PREFIX}'.\")\n",
    "    doc.add_heading('Global Aggregate Statistics (Final Scores)', level=1)\n",
    "    kc_stats = final_scores_df[all_kcs].describe().loc[['mean', 'min', 'max']]\n",
    "    metric_stats = final_scores_df[all_metrics].describe().loc[['mean', 'min', 'max']]\n",
    "    add_df_to_doc(doc, kc_stats.T, \"Global KC Score Statistics\")\n",
    "    add_df_to_doc(doc, metric_stats.T, \"Global Metric Value Statistics\")\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Leaderboards', level=1)\n",
    "    final_scores_df = final_scores_df.set_index('user_name')\n",
    "    final_scores_df['Total KC Score'] = final_scores_df[all_kcs].sum(axis=1)\n",
    "    add_df_to_doc(doc, final_scores_df[['Total KC Score']].sort_values('Total KC Score', ascending=False).head(10), \"Top 10 - KC Leaderboard\")\n",
    "    try:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_metrics = scaler.fit_transform(final_scores_df[all_metrics])\n",
    "        final_scores_df['Composite Metric Score'] = pd.DataFrame(normalized_metrics, index=final_scores_df.index).sum(axis=1)\n",
    "        doc.add_paragraph(\"The Metric Leaderboard is calculated by normalizing each metric globally and summing these scores.\")\n",
    "        add_df_to_doc(doc, final_scores_df[['Composite Metric Score']].sort_values('Composite Metric Score', ascending=False).head(10), \"Top 10 - Metric Leaderboard\")\n",
    "    except ImportError:\n",
    "        doc.add_paragraph(\"Metric Leaderboard requires scikit-learn: pip install scikit-learn\")\n",
    "    doc.save(output_dir / \"Master_Report.docx\")\n",
    "    print(\"   Master Report saved.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting report generation script...\")\n",
    "    load_dotenv('.env.local')\n",
    "    supabase_url = os.environ.get(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "    supabase_key = os.environ.get(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "    if not supabase_url or not supabase_key:\n",
    "        print(\"Error: Supabase credentials not found. Check .env.local file.\")\n",
    "        return\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    supabase: Client = create_client(supabase_url, supabase_key)\n",
    "    raw_df = fetch_analytics_data(supabase)\n",
    "    if raw_df.empty: return\n",
    "    all_learners_data = process_learner_data(raw_df)\n",
    "    if not all_learners_data: return\n",
    "    per_goal_stats, final_scores_df = calculate_all_stats(all_learners_data)\n",
    "    print(\"\\nGenerating individual learner reports...\")\n",
    "    for data in all_learners_data.values():\n",
    "        generate_individual_report(data, per_goal_stats, OUTPUT_DIR)\n",
    "    print(\"\\nGenerating master report...\")\n",
    "    first_learner = all_learners_data[list(all_learners_data.keys())[0]]\n",
    "    generate_master_report(\n",
    "        final_scores_df, \n",
    "        first_learner['all_kcs'], \n",
    "        first_learner['all_metrics'], \n",
    "        len(all_learners_data), \n",
    "        OUTPUT_DIR\n",
    "    )\n",
    "    print(f\"\\nâœ… All reports have been generated and saved in the '{OUTPUT_DIR}' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting report generation script...\n",
      "Fetching data for learners starting with 'BT'...\n",
      "Successfully fetched 107 records.\n",
      "Processing raw data...\n",
      "  -> Processing data for BT8BT9 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT15BT16 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT14 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT2 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT4 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT13 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT7 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT1 on Goal: 'Break Even and Build Trust'\n",
      "  -> Processing data for BT11BT12 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT7 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT5 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT3+BT6 on Goal: 'Price with Purpose'\n",
      "  -> Processing data for BT10 on Goal: 'Break Even and Build Trust'\n",
      "Data processing complete.\n",
      "Calculating statistics per goal...\n",
      "  -> Calculating stats for goal: 'Break Even and Build Trust'\n",
      "  -> Calculating stats for goal: 'Price with Purpose'\n",
      "\n",
      "Generating individual learner reports...\n",
      "  -> Generating report for BT8BT9 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT15BT16 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT14 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT2 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT4 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT13 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT7 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT1 (Goal: Break Even and Build Trust)...\n",
      "  -> Generating report for BT11BT12 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT7 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT5 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT3+BT6 (Goal: Price with Purpose)...\n",
      "  -> Generating report for BT10 (Goal: Break Even and Build Trust)...\n",
      "\n",
      "Generating master report...\n",
      "-> Generating Master Report...\n",
      "   Master Report saved.\n",
      "\n",
      "âœ… All reports have been generated and saved in the 'GeneratedReports' folder.\n"
     ]
    }
   ],
   "source": [
    "# generate_reports.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Data and DB Libraries\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Reporting and Plotting Libraries\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = Path(\"GeneratedReports\")\n",
    "LEARNER_PREFIX = \"BT\"\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def add_df_to_doc(doc, df, title=\"\"):\n",
    "    if title:\n",
    "        doc.add_heading(title, level=2)\n",
    "    if df.empty:\n",
    "        doc.add_paragraph(\"No data available.\")\n",
    "        return\n",
    "    df_rounded = df.copy()\n",
    "    for col in df_rounded.select_dtypes(include=['number']).columns:\n",
    "        df_rounded[col] = df_rounded[col].round(2)\n",
    "    if isinstance(df_rounded.index, pd.RangeIndex):\n",
    "        df_for_table = df_rounded.reset_index(drop=True)\n",
    "    else:\n",
    "        df_for_table = df_rounded.reset_index()\n",
    "    table = doc.add_table(rows=1, cols=len(df_for_table.columns))\n",
    "    table.style = 'Table Grid'\n",
    "    for j, col_name in enumerate(df_for_table.columns):\n",
    "        table.cell(0, j).text = str(col_name)\n",
    "    for i, row in df_for_table.iterrows():\n",
    "        row_cells = table.add_row().cells\n",
    "        for j, val in enumerate(row):\n",
    "            row_cells[j].text = str(val)\n",
    "\n",
    "def create_plot_from_df(df, title, y_label, x_col='Decision Point'):\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(10, 5.5))\n",
    "    for col in df.columns:\n",
    "        if col != x_col:\n",
    "            ax.plot(df[x_col], df[col], marker='o', linestyle='-', label=col)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(x_col, fontsize=12)\n",
    "    ax.set_ylabel(y_label, fontsize=12)\n",
    "    # Add a legend only if there is more than one data series to plot\n",
    "    if len(df.columns) > 2:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        fig.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    img_stream = io.BytesIO()\n",
    "    plt.savefig(img_stream, format='png', dpi=300)\n",
    "    plt.close(fig)\n",
    "    img_stream.seek(0)\n",
    "    return img_stream\n",
    "\n",
    "# --- CORE SCRIPT LOGIC ---\n",
    "\n",
    "def fetch_analytics_data(supabase: Client) -> pd.DataFrame:\n",
    "    print(f\"Fetching data for learners starting with '{LEARNER_PREFIX}'...\")\n",
    "    try:\n",
    "        response = supabase.table('historical_learning_analytics').select(\n",
    "            'created_at, user_id, goal_id, scenario_attempt_number, decision_number, '\n",
    "            'kc_scores_after_decision, metric_values_after_decision, '\n",
    "            'users!inner(name), goals!inner(name)'\n",
    "        ).like('users.name', f'{LEARNER_PREFIX}%').order('user_id').order('created_at', desc=False).execute()\n",
    "        if not response.data:\n",
    "            print(\"No data found for the specified learners.\")\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame(response.data)\n",
    "        df['user_name'] = df['users'].apply(lambda x: x['name'])\n",
    "        df['goal_name'] = df['goals'].apply(lambda x: x['name'])\n",
    "        df = df.drop(columns=['users', 'goals'])\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "        print(f\"Successfully fetched {len(df)} records.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def process_learner_data(raw_df: pd.DataFrame) -> dict:\n",
    "    print(\"Processing raw data...\")\n",
    "    if raw_df.empty: return {}\n",
    "    all_learners_data = {}\n",
    "    all_kcs = set()\n",
    "    all_metrics = set()\n",
    "    raw_df['kc_scores_after_decision'].dropna().apply(lambda x: all_kcs.update(x.keys()))\n",
    "    raw_df['metric_values_after_decision'].dropna().apply(lambda x: all_metrics.update(x.keys()))\n",
    "    all_kcs, all_metrics = sorted(list(all_kcs)), sorted(list(all_metrics))\n",
    "    for (user_id, goal_id), group in raw_df.groupby(['user_id', 'goal_id']):\n",
    "        user_name = group['user_name'].iloc[0]\n",
    "        goal_name = group['goal_name'].iloc[0]\n",
    "        print(f\"  -> Processing data for {user_name} on Goal: '{goal_name}'\")\n",
    "        learner_df = group.sort_values('created_at').reset_index(drop=True)\n",
    "        kc_df = pd.json_normalize(learner_df['kc_scores_after_decision']).reindex(columns=all_kcs).fillna(0)\n",
    "        metric_df = pd.json_normalize(learner_df['metric_values_after_decision']).reindex(columns=all_metrics).fillna(0)\n",
    "        learner_df['Decision Point'] = [f\"G{r.goal_id}-S{r.scenario_attempt_number}-D{r.decision_number}\" for r in learner_df.itertuples()]\n",
    "        genesis_row = {'created_at': learner_df['created_at'].iloc[0] - timedelta(seconds=1), 'Decision Point': 'Genesis', **{kc: 0 for kc in all_kcs}, **{metric: 0 for metric in all_metrics}}\n",
    "        genesis_df = pd.DataFrame([genesis_row])\n",
    "        processed_df = pd.concat([learner_df[['created_at', 'Decision Point']], kc_df, metric_df], axis=1)\n",
    "        full_df = pd.concat([genesis_df, processed_df], ignore_index=True)\n",
    "        final_scores = full_df.iloc[-1]\n",
    "        active_kcs = [kc for kc in all_kcs if final_scores[kc] != 0]\n",
    "        active_metrics = [metric for metric in all_metrics if final_scores[metric] != 0]\n",
    "        all_learners_data[f\"{user_id}_{goal_id}\"] = {\"user_name\": user_name, \"goal_name\": goal_name, \"full_data\": full_df, \"all_kcs\": all_kcs, \"all_metrics\": all_metrics, \"active_kcs\": active_kcs, \"active_metrics\": active_metrics}\n",
    "    print(\"Data processing complete.\")\n",
    "    return all_learners_data\n",
    "\n",
    "def calculate_all_stats(all_learners_data: dict):\n",
    "    print(\"Calculating statistics per goal...\")\n",
    "    if not all_learners_data:\n",
    "        return {}, pd.DataFrame()\n",
    "    final_scores_list = []\n",
    "    for data in all_learners_data.values():\n",
    "        final_row = data['full_data'].iloc[-1].copy()\n",
    "        final_row['user_name'] = data['user_name']\n",
    "        final_row['goal_name'] = data['goal_name']\n",
    "        final_scores_list.append(final_row)\n",
    "    final_scores_df = pd.DataFrame(final_scores_list)\n",
    "    all_kcs = all_learners_data[list(all_learners_data.keys())[0]]['all_kcs']\n",
    "    all_metrics = all_learners_data[list(all_learners_data.keys())[0]]['all_metrics']\n",
    "    per_goal_stats = {}\n",
    "    for goal_name, group_df in final_scores_df.groupby('goal_name'):\n",
    "        print(f\"  -> Calculating stats for goal: '{goal_name}'\")\n",
    "        kc_stats = group_df[all_kcs].describe().loc[['mean', 'min', 'max']]\n",
    "        metric_stats = group_df[all_metrics].describe().loc[['mean', 'min', 'max']]\n",
    "        per_goal_stats[goal_name] = {'kc': kc_stats, 'metric': metric_stats}\n",
    "    return per_goal_stats, final_scores_df\n",
    "\n",
    "def generate_individual_report(learner_data: dict, per_goal_stats: dict, output_dir: Path):\n",
    "    \"\"\"Generates an individual report with goal-specific stats and smart, split metric plots.\"\"\"\n",
    "    user_name = learner_data['user_name']\n",
    "    goal_name = learner_data['goal_name']\n",
    "    df = learner_data['full_data']\n",
    "    active_kcs = learner_data['active_kcs']\n",
    "    active_metrics = learner_data['active_metrics']\n",
    "\n",
    "    print(f\"  -> Generating report for {user_name} (Goal: {goal_name})...\")\n",
    "    \n",
    "    doc = Document()\n",
    "    doc.add_heading(f'Learner Report: {user_name}', 0)\n",
    "    doc.add_paragraph(f\"Goal Attempted: {goal_name}\", style='Quote')\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    doc.add_heading('Final Cumulative Scores', level=1)\n",
    "    doc.add_paragraph(\n",
    "        \"This section shows the learner's final score compared to the average, min, and max \"\n",
    "        \"of all learners who attempted THIS SAME GOAL.\"\n",
    "    )\n",
    "    \n",
    "    final_scores = df.iloc[-1]\n",
    "    specific_goal_stats = per_goal_stats.get(goal_name, {})\n",
    "\n",
    "    if specific_goal_stats:\n",
    "        if active_kcs:\n",
    "            kc_summary = final_scores[active_kcs].to_frame(name='Your Score')\n",
    "            kc_summary = kc_summary.join(specific_goal_stats['kc'][active_kcs].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "            add_df_to_doc(doc, kc_summary, \"Final KC Scores\")\n",
    "        if active_metrics:\n",
    "            metric_summary = final_scores[active_metrics].to_frame(name='Your Value')\n",
    "            metric_summary = metric_summary.join(specific_goal_stats['metric'][active_metrics].T.rename(columns={'mean': 'Class Avg', 'min': 'Class Min', 'max': 'Class Max'}))\n",
    "            add_df_to_doc(doc, metric_summary, \"Final Metric Values\")\n",
    "\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Performance Over Time', level=1)\n",
    "    \n",
    "    if active_kcs:\n",
    "        kc_progression_df = df[['Decision Point'] + active_kcs]\n",
    "        add_df_to_doc(doc, kc_progression_df.set_index('Decision Point'), \"KC Scores per Decision Point\")\n",
    "        kc_plot_img = create_plot_from_df(kc_progression_df, \"KC Scores Over Time\", \"KC Score\")\n",
    "        doc.add_picture(kc_plot_img, width=Inches(6.5))\n",
    "    \n",
    "    if active_metrics:\n",
    "        metric_progression_df = df[['Decision Point'] + active_metrics]\n",
    "        add_df_to_doc(doc, metric_progression_df.set_index('Decision Point'), \"Metric Values per Decision Point\")\n",
    "\n",
    "        if 'Revenue' in active_metrics:\n",
    "            revenue_df = df[['Decision Point', 'Revenue']]\n",
    "            revenue_plot_img = create_plot_from_df(revenue_df, \"Revenue Over Time\", \"Revenue Value\")\n",
    "            doc.add_heading(\"Revenue Progression\", level=3)\n",
    "            doc.add_picture(revenue_plot_img, width=Inches(6.5))\n",
    "\n",
    "        if 'Reputation' in active_metrics:\n",
    "            reputation_df = df[['Decision Point', 'Reputation']]\n",
    "            rep_plot_img = create_plot_from_df(reputation_df, \"Reputation Over Time\", \"Reputation (1-5 scale)\")\n",
    "            doc.add_heading(\"Reputation Progression\", level=3)\n",
    "            doc.add_picture(rep_plot_img, width=Inches(6.5))\n",
    "\n",
    "        # --- FINAL REFINED PLOTTING LOGIC ---\n",
    "        other_metrics = [m for m in ['Customer Satisfaction', 'Risk-Taking', 'Ethical Decision Making'] if m in active_metrics]\n",
    "        if other_metrics:\n",
    "            other_metrics_df = df[['Decision Point'] + other_metrics]\n",
    "            \n",
    "            # If only one metric, be specific. Otherwise, be generic and use a legend.\n",
    "            if len(other_metrics) == 1:\n",
    "                metric_name = other_metrics[0]\n",
    "                plot_title = f\"{metric_name} Progression\"\n",
    "                y_label = f\"{metric_name} (0-100 scale)\"\n",
    "            else:\n",
    "                plot_title = \"High-Scale Metrics Progression\"\n",
    "                y_label = \"Metric Value (0-100 scale)\"\n",
    "            \n",
    "            doc.add_heading(plot_title, level=3)\n",
    "            other_plot_img = create_plot_from_df(other_metrics_df, plot_title, y_label)\n",
    "            doc.add_picture(other_plot_img, width=Inches(6.5))\n",
    "\n",
    "    file_path = output_dir / f\"Report_{user_name.replace(' ', '_')}_{goal_name.replace(' ', '_')}.docx\"\n",
    "    doc.save(file_path)\n",
    "\n",
    "\n",
    "def generate_master_report(final_scores_df: pd.DataFrame, all_kcs: list, all_metrics: list, num_learners: int, output_dir: Path):\n",
    "    print(\"-> Generating Master Report...\")\n",
    "    if final_scores_df.empty:\n",
    "        print(\"   No data available to generate a master report.\")\n",
    "        return\n",
    "    doc = Document()\n",
    "    doc.add_heading('Master Learner Report', 0)\n",
    "    doc.add_paragraph(f\"Report generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    doc.add_paragraph(f\"This report covers {num_learners} learner-goal attempts for names starting with '{LEARNER_PREFIX}'.\")\n",
    "    doc.add_heading('Global Aggregate Statistics (Final Scores)', level=1)\n",
    "    kc_stats = final_scores_df[all_kcs].describe().loc[['mean', 'min', 'max']]\n",
    "    metric_stats = final_scores_df[all_metrics].describe().loc[['mean', 'min', 'max']]\n",
    "    add_df_to_doc(doc, kc_stats.T, \"Global KC Score Statistics\")\n",
    "    add_df_to_doc(doc, metric_stats.T, \"Global Metric Value Statistics\")\n",
    "    doc.add_page_break()\n",
    "    doc.add_heading('Leaderboards', level=1)\n",
    "    final_scores_df = final_scores_df.set_index('user_name')\n",
    "    final_scores_df['Total KC Score'] = final_scores_df[all_kcs].sum(axis=1)\n",
    "    add_df_to_doc(doc, final_scores_df[['Total KC Score']].sort_values('Total KC Score', ascending=False).head(10), \"Top 10 - KC Leaderboard\")\n",
    "    try:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        normalized_metrics = scaler.fit_transform(final_scores_df[all_metrics])\n",
    "        final_scores_df['Composite Metric Score'] = pd.DataFrame(normalized_metrics, index=final_scores_df.index).sum(axis=1)\n",
    "        doc.add_paragraph(\"The Metric Leaderboard is calculated by normalizing each metric globally and summing these scores.\")\n",
    "        add_df_to_doc(doc, final_scores_df[['Composite Metric Score']].sort_values('Composite Metric Score', ascending=False).head(10), \"Top 10 - Metric Leaderboard\")\n",
    "    except ImportError:\n",
    "        doc.add_paragraph(\"Metric Leaderboard requires scikit-learn: pip install scikit-learn\")\n",
    "    doc.save(output_dir / \"Master_Report.docx\")\n",
    "    print(\"   Master Report saved.\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting report generation script...\")\n",
    "    load_dotenv('.env.local')\n",
    "    supabase_url = os.environ.get(\"NEXT_PUBLIC_SUPABASE_URL\")\n",
    "    supabase_key = os.environ.get(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
    "    if not supabase_url or not supabase_key:\n",
    "        print(\"Error: Supabase credentials not found. Check .env.local file.\")\n",
    "        return\n",
    "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "    supabase: Client = create_client(supabase_url, supabase_key)\n",
    "    raw_df = fetch_analytics_data(supabase)\n",
    "    if raw_df.empty: return\n",
    "    all_learners_data = process_learner_data(raw_df)\n",
    "    if not all_learners_data: return\n",
    "    per_goal_stats, final_scores_df = calculate_all_stats(all_learners_data)\n",
    "    print(\"\\nGenerating individual learner reports...\")\n",
    "    for data in all_learners_data.values():\n",
    "        generate_individual_report(data, per_goal_stats, OUTPUT_DIR)\n",
    "    print(\"\\nGenerating master report...\")\n",
    "    first_learner = all_learners_data[list(all_learners_data.keys())[0]]\n",
    "    generate_master_report(\n",
    "        final_scores_df, \n",
    "        first_learner['all_kcs'], \n",
    "        first_learner['all_metrics'], \n",
    "        len(all_learners_data), \n",
    "        OUTPUT_DIR\n",
    "    )\n",
    "    print(f\"\\nâœ… All reports have been generated and saved in the '{OUTPUT_DIR}' folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
